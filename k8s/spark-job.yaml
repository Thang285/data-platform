apiVersion: sparkoperator.k8s.io/v1beta2
kind: SparkApplication
metadata:
  name: cicd-orders-job
  namespace: compute
spec:
  type: Python
  pythonVersion: "3"
  mode: cluster
  image: "public.ecr.aws/bitnami/spark:3.5.1-debian-12-r11"
  imagePullPolicy: IfNotPresent
  mainApplicationFile: "{{CODE_PATH}}"
  sparkVersion: "3.5.1"
  restartPolicy:
    type: OnFailure
    onFailureRetries: 3

  # ⚡ FIX: Use native /tmp (No volumes attached)
  sparkConf:
    "spark.kubernetes.executor.deleteOnTermination": "false"
    "spark.jars.ivy": "/tmp/.ivy2"
    "spark.driver.memoryOverhead": "128m"
    "spark.executor.memoryOverhead": "128m"

  deps:
    packages:
      - org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.1
      - org.apache.hadoop:hadoop-aws:3.3.4
      - com.amazonaws:aws-java-sdk-bundle:1.12.262

  # ⚙️ DRIVER CONFIG
  driver:
    cores: 1
    coreLimit: "1200m"  # <--- UPDATED: Must be >= cores (1)
    memory: "512m"
    serviceAccount: spark-spark-operator-spark
    # Run Driver as Root to handle internal logic
    securityContext:
      runAsUser: 0
    env:
      - name: AWS_ACCESS_KEY_ID
        value: "minioadmin"
      - name: AWS_SECRET_ACCESS_KEY
        value: "minioadminpassword"
      - name: AWS_REGION
        value: "us-east-1"
  
  # ⚙️ EXECUTOR CONFIG
  executor:
    cores: 1
    coreLimit: "1200m"  # <--- UPDATED: Must be >= cores (1)
    instances: 1
    memory: "512m"
    securityContext:
      runAsUser: 0
    env:
      - name: AWS_ACCESS_KEY_ID
        value: "minioadmin"
      - name: AWS_SECRET_ACCESS_KEY
        value: "minioadminpassword"

  # HADOOP CONFIG
  hadoopConf:
    "fs.s3a.endpoint": "http://minio.data.svc.cluster.local:9000"
    "fs.s3a.access.key": "minioadmin"
    "fs.s3a.secret.key": "minioadminpassword"
    "fs.s3a.path.style.access": "true"
    "fs.s3a.impl": "org.apache.hadoop.fs.s3a.S3AFileSystem"
    "fs.s3a.connection.ssl.enabled": "false"
    "fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider"