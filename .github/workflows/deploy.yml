name: Deploy Data Pipeline

on:
  push:
    branches: [ "main" ]
    paths:
      - 'src/**'
      - 'k8s/**'

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      # 1. Setup SSH Connection
      - name: Setup SSH
        run: |
          mkdir -p ~/.ssh/
          echo "${{ secrets.VPS_SSH_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -H ${{ secrets.VPS_HOST }} >> ~/.ssh/known_hosts

      # 2. Copy Files to VPS (/tmp folder)
      - name: Copy Files to VPS
        run: |
          # Create a Short SHA for versioning
          SHA_SHORT=$(echo "${{ github.sha }}" | cut -c1-7)
          scp src/jobs/orders_etl.py ${{ secrets.VPS_USER }}@${{ secrets.VPS_HOST }}:/tmp/orders_etl_${SHA_SHORT}.py
          scp k8s/spark-job.yaml ${{ secrets.VPS_USER }}@${{ secrets.VPS_HOST }}:/tmp/spark-job.yaml

      # 3. Remote Execution on VPS
      - name: Deploy on Kubernetes
        run: |
          ssh ${{ secrets.VPS_USER }}@${{ secrets.VPS_HOST }} << 'ENDSSH'
            set -e
            echo "ðŸ”¥ STARTING DEPLOYMENT..."
            
            # VARIABLES
            SHA_SHORT=$(echo "${{ github.sha }}" | cut -c1-7)
            POD_NAME="uploader-${SHA_SHORT}"
            REMOTE_PATH="s3a://lakehouse/code/${SHA_SHORT}/orders_etl.py"

            echo "ðŸ†” Unique Pod Name: $POD_NAME"

            # A. Start Helper Pod 
            # FIX: Use Amazon ECR to avoid Docker Hub limits
            # FIX: Use 'bitnami/minio-client' because it contains 'tar' (required for kubectl cp)
            kubectl run $POD_NAME \
              --image=public.ecr.aws/bitnami/minio-client:latest \
              --image-pull-policy=IfNotPresent \
              --restart=Never \
              --command -- sleep 600
            
            echo "â³ Waiting for helper pod..."
            kubectl wait --for=condition=ready pod/$POD_NAME --timeout=120s
            
            # B. Setup MinIO Credentials inside the pod
            kubectl exec $POD_NAME -- mc alias set myminio http://minio.data.svc.cluster.local:9000 minioadmin minioadminpassword

            # C. Upload Code (VPS -> Pod -> MinIO)
            echo "ðŸ“¦ Uploading code to: $REMOTE_PATH"
            kubectl cp /tmp/orders_etl_${SHA_SHORT}.py $POD_NAME:/tmp/etl.py
            kubectl exec $POD_NAME -- mc cp /tmp/etl.py myminio/lakehouse/code/${SHA_SHORT}/orders_etl.py

            # D. Update Kubernetes Manifest
            # Inject the MinIO URL into the template
            sed "s|{{CODE_PATH}}|$REMOTE_PATH|g" /tmp/spark-job.yaml > /tmp/final-job.yaml
            
            # E. Apply Job
            # Delete old job first to force a restart
            kubectl delete sparkapplication cicd-orders-job -n compute --ignore-not-found
            kubectl apply -f /tmp/final-job.yaml
            
            # F. Cleanup
            echo "ðŸ§¹ Cleaning up..."
            kubectl delete pod $POD_NAME --grace-period=0 --force
            rm /tmp/orders_etl_*.py /tmp/spark-job.yaml /tmp/final-job.yaml
            
            echo "âœ… DEPLOYMENT COMPLETE!"
          ENDSSH