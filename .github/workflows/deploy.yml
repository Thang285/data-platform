name: Deploy Data Pipeline

on:
  push:
    branches: [ "main" ]
    paths:
      - 'src/**'
      - 'k8s/**'

jobs:
  deploy:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Setup SSH
        run: |
          mkdir -p ~/.ssh/
          echo "${{ secrets.VPS_SSH_KEY }}" > ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          ssh-keyscan -H ${{ secrets.VPS_HOST }} >> ~/.ssh/known_hosts

      - name: Deploy to VPS
        run: |
          # 1. Copy Files to VPS
          # We use the Short SHA to make filenames unique
          SHA_SHORT=$(echo "${{ github.sha }}" | cut -c1-7)
          scp src/jobs/orders_etl.py ${{ secrets.VPS_USER }}@${{ secrets.VPS_HOST }}:/tmp/orders_etl_${SHA_SHORT}.py
          scp k8s/spark-job.yaml ${{ secrets.VPS_USER }}@${{ secrets.VPS_HOST }}:/tmp/spark-job.yaml

          # 2. Remote Execution
          ssh ${{ secrets.VPS_USER }}@${{ secrets.VPS_HOST }} << 'ENDSSH'
            set -e
            echo "ðŸ”¥ STARTING DEPLOYMENT..."
            
            # VARIABLES
            SHA_SHORT=$(echo "${{ github.sha }}" | cut -c1-7)
            POD_NAME="uploader-${SHA_SHORT}"
            REMOTE_PATH="s3a://lakehouse/code/${SHA_SHORT}/orders_etl.py"

            echo "ðŸ†” Using Unique Pod Name: $POD_NAME"

            # A. Start Helper Pod (Bitnami image has 'tar' installed)
            kubectl run $POD_NAME --image=bitnami/minio-client --restart=Never --command -- sleep 600
            echo "â³ Waiting for pod to be ready..."
            kubectl wait --for=condition=ready pod/$POD_NAME --timeout=60s
            
            # B. Setup MinIO Creds
            kubectl exec $POD_NAME -- mc alias set myminio http://minio.data.svc.cluster.local:9000 minioadmin minioadminpassword

            # C. Upload Code
            echo "ðŸ“¦ Uploading to: $REMOTE_PATH"
            kubectl cp /tmp/orders_etl_${SHA_SHORT}.py $POD_NAME:/tmp/etl.py
            kubectl exec $POD_NAME -- mc cp /tmp/etl.py myminio/lakehouse/code/${SHA_SHORT}/orders_etl.py

            # D. Update Manifest
            sed "s|{{CODE_PATH}}|$REMOTE_PATH|g" /tmp/spark-job.yaml > /tmp/final-job.yaml
            
            # E. Apply Job (Delete old one first)
            kubectl delete sparkapplication cicd-orders-job -n compute --ignore-not-found
            kubectl apply -f /tmp/final-job.yaml
            
            # F. Cleanup
            echo "ðŸ§¹ Cleaning up..."
            kubectl delete pod $POD_NAME --grace-period=0 --force
            rm /tmp/orders_etl_*.py /tmp/spark-job.yaml /tmp/final-job.yaml
            
            echo "âœ… DEPLOYMENT COMPLETE!"
          ENDSSH